# train_psf_all_cnn.py
# PSF 1~8 ì „ì²´ì— ëŒ€í•´ PSFë³„ ë¼ë²¨/ìˆ˜ì¤€ì„ ë°˜ì˜í•œ 1D-CNN í•™ìŠµ/í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

import json
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow.keras import models, layers, callbacks

# ì¬í˜„ì„±(ëœë¤ ê³ ì •)
np.random.seed(42)
tf.random.set_seed(42)

# --------------------------------------------------------
# 0. CNN ëª¨ë¸ ìƒì„± í•¨ìˆ˜
# --------------------------------------------------------
def build_cnn_model(n_features: int, n_classes: int):
    """
    n_features: feature ê°œìˆ˜ (ì‹œí€€ìŠ¤ ê¸¸ì´)
    n_classes: ë¼ë²¨ ê°œìˆ˜ (2 or 3)
    """
    model = models.Sequential()
    model.add(layers.Input(shape=(n_features, 1)))     # (ê¸¸ì´, ì±„ë„=1)

    # 1ë²ˆì§¸ Conv ë¸”ë¡
    model.add(layers.Conv1D(filters=64, kernel_size=3, padding="same", activation="relu"))
    model.add(layers.MaxPooling1D(pool_size=2))

    # 2ë²ˆì§¸ Conv ë¸”ë¡
    model.add(layers.Conv1D(filters=128, kernel_size=3, padding="same", activation="relu"))
    model.add(layers.MaxPooling1D(pool_size=2))

    # ì¶œë ¥ë¶€
    model.add(layers.GlobalAveragePooling1D())
    model.add(layers.Dropout(0.5))

    if n_classes == 2:
        # ì´ì§„ ë¶„ë¥˜ (PSF 5, 6)
        model.add(layers.Dense(1, activation="sigmoid"))
        loss = "binary_crossentropy"
    else:
        # ë‹¤ì¤‘ ë¶„ë¥˜ (PSF 1,2,3,4,7,8)
        model.add(layers.Dense(n_classes, activation="softmax"))
        loss = "sparse_categorical_crossentropy"

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss=loss,
        metrics=["accuracy"],
    )
    return model


# --------------------------------------------------------
# 1. PSFë³„ ë¼ë²¨ ë§¤í•‘ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)
# --------------------------------------------------------
label_map_by_psf = {
    1: {  # PSF1: ë³´í†µ / ë‚˜ì¨ / ë§¤ìš°ë‚˜ì¨
        "ë³´í†µ": 0,
        "ë‚˜ì¨": 1,
        "ë§¤ìš°ë‚˜ì¨": 2,
    },
    2: {  # PSF2: ì¢‹ìŒ / ë³´í†µ / ë‚˜ì¨
        "ì¢‹ìŒ": 0,
        "ë³´í†µ": 1,
        "ë‚˜ì¨": 2,
    },
    3: {  # PSF3: ì¢‹ìŒ / ë³´í†µ / ë‚˜ì¨
        "ì¢‹ìŒ": 0,
        "ë³´í†µ": 1,
        "ë‚˜ì¨": 2,
    },
    4: {  # PSF4: êµìœ¡/í›ˆë ¨ (ì¢‹ìŒ / ë³´í†µ / ë‚˜ì¨)
        "ì¢‹ìŒ": 0,
        "ë³´í†µ": 1,
        "ë‚˜ì¨": 2,
    },
    5: {  # PSF5: 2ë‹¨ê³„
        "ë³´í†µ": 0,
        "ë‚˜ì¨": 1,
    },
    6: {  # PSF6: 2ë‹¨ê³„
        "ë³´í†µ": 0,
        "ë‚˜ì¨": 1,
    },
    7: {  # PSF7: ì¢‹ìŒ / ë³´í†µ / ë‚˜ì¨
        "ì¢‹ìŒ": 0,
        "ë³´í†µ": 1,
        "ë‚˜ì¨": 2,
    },
    8: {  # PSF8: ë³´í†µ / ì•½ê°„ ë†’ìŒ / ë§¤ìš° ë†’ìŒ
        "ë³´í†µ": 0,
        "ì•½ê°„ ë†’ìŒ": 1,
        "ë§¤ìš° ë†’ìŒ": 2,
    },
}

# 3ë‹¨ê³„ìš© ê¸°ë³¸ í…ìŠ¤íŠ¸ (PSF 1,2,3,4,7,8)
default_severity_text = {
    0: "ì¢‹ìŒ(ìˆ˜ì¤€ 1)",
    1: "ë³´í†µ(ìˆ˜ì¤€ 2)",
    2: "ë‚˜ì¨(ìˆ˜ì¤€ 3)",
}

# PSF 5,6 ì „ìš© í…ìŠ¤íŠ¸ (2ë‹¨ê³„)
severity_text_by_psf = {
    5: {
        0: "ë³´í†µ(ìˆ˜ì¤€ 1)",
        1: "ë‚˜ì¨(ìˆ˜ì¤€ 2)",
    },
    6: {
        0: "ë³´í†µ(ìˆ˜ì¤€ 1)",
        1: "ë‚˜ì¨(ìˆ˜ì¤€ 2)",
    },
}

# --------------------------------------------------------
# 2. JSON íŒŒì¼ ì½ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)
# --------------------------------------------------------
with open("ì„¤ë¬¸ì¡°ì‚¬ê²°ê³¼.txt", "r", encoding="utf-8") as f:
    data = json.load(f)

rows = []

for resp in data:
    rnd = resp.get("round")
    for q in resp["questions"]:
        psf_id = q.get("id")
        answer_text = q.get("answer")

        # 1~8ë²ˆ PSF ì¤‘ ì•„ë‹Œ ê±´ ë¬´ì‹œ
        if psf_id not in label_map_by_psf:
            continue

        label_map = label_map_by_psf[psf_id]
        if answer_text not in label_map:
            continue

        row = {
            "psf_id": psf_id,
            "round": rnd,
            "answer_text": answer_text,
            "label": label_map[answer_text],
        }

        # í•˜ìœ„ PSF ì¡°ê±´ë“¤ ì¶”ê°€
        conds = q.get("conditions", {})
        for k, v in conds.items():
            row[k] = v

        rows.append(row)

df = pd.DataFrame(rows)
print("ì „ì²´ PSF ì§ˆë¬¸ ìˆ˜:", len(df))
print("PSFë³„ ë°ì´í„° ê°œìˆ˜:")
print(df["psf_id"].value_counts().sort_index())

# --------------------------------------------------------
# 3. PSFë³„ë¡œ CNN í•™ìŠµ/í‰ê°€
# --------------------------------------------------------
for psf_id in sorted(df["psf_id"].unique()):
    print("\n" + "=" * 70)
    print(f"=== PSF {psf_id} í•™ìŠµ/í‰ê°€ (CNN) ===")

    df_psf = df[df["psf_id"] == psf_id].copy()
    print("ìƒ˜í”Œ ìˆ˜:", len(df_psf))
    print("ë¼ë²¨ ë¶„í¬:")
    print(df_psf["label"].value_counts().sort_index())

    # ë¼ë²¨ì´ 1ì¢…ë¥˜ë¿ì´ë©´ í•™ìŠµ ë¶ˆê°€
    if df_psf["label"].nunique() < 2:
        print("âš ï¸ ë¼ë²¨ì´ í•œ ì¢…ë¥˜ë¿ì´ë¼ í•™ìŠµ/í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    # ì´ PSFì—ì„œ ì‹¤ì œ ê°’ì´ ìˆëŠ” feature ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    drop_cols = ["psf_id", "round", "answer_text", "label"]
    candidate_cols = [c for c in df_psf.columns if c not in drop_cols]
    feature_cols = [c for c in candidate_cols if df_psf[c].notna().any()]

    if not feature_cols:
        print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ feature(ì¡°ê±´)ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    # ì¡°ê±´ê°’ ë¬¸ìì—´ -> ì¹´í…Œê³ ë¦¬ ì½”ë“œ(0,1,2,...)ë¡œ ë³€í™˜ (ê¸°ì¡´ê³¼ ë™ì¼)
    for col in feature_cols:
        df_psf[col] = df_psf[col].astype("category").cat.codes

    # -1 (ê²°ì¸¡) í¬í•¨ëœ í–‰ ì œê±°
    mask_valid = (df_psf[feature_cols] != -1).all(axis=1)
    df_psf = df_psf[mask_valid].copy()

    if len(df_psf) < 5:
        print("âš ï¸ ìœ íš¨í•œ ìƒ˜í”Œì´ ë„ˆë¬´ ì ì–´(5ê°œ ë¯¸ë§Œ) í•™ìŠµ/í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    # ì…ë ¥/ë¼ë²¨ ë¶„ë¦¬
    X = df_psf[feature_cols].values.astype(np.float32)
    y = df_psf["label"].values.astype(np.int64)

    # ----------------------------------------------------
    # 3-1. train / test ë¶„ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
    # ----------------------------------------------------
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
            stratify=y,
        )
    except ValueError:
        # ë°ì´í„°ê°€ ì ì–´ì„œ stratify ì‹¤íŒ¨ ì‹œ
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=0.2,
            random_state=42,
        )
        print("âš ï¸ stratify ì—†ì´ train/test ë¶„ë¦¬ ìˆ˜í–‰")

    # ----------------------------------------------------
    # 3-2. ìŠ¤ì¼€ì¼ë§ + CNN ì…ë ¥ í˜•íƒœë¡œ reshape
    # ----------------------------------------------------
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # (samples, features) â†’ (samples, features, 1)
    X_train_cnn = X_train_scaled[..., np.newaxis]
    X_test_cnn = X_test_scaled[..., np.newaxis]

    n_features = X_train_cnn.shape[1]
    n_classes = len(np.unique(y))

    print(f"feature ê°œìˆ˜: {n_features}, í´ë˜ìŠ¤ ê°œìˆ˜: {n_classes}")

    # ----------------------------------------------------
    # 3-3. CNN ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
    # ----------------------------------------------------
    model = build_cnn_model(n_features, n_classes)
    model.summary(print_fn=lambda x: None)  # ì½˜ì†”ì— summary ìŸì•„ì§€ëŠ” ê²Œ ì‹«ìœ¼ë©´ ì´ë ‡ê²Œ

    early_stop = callbacks.EarlyStopping(
        monitor="val_loss",
        patience=10,
        restore_best_weights=True,
        verbose=1,
    )

    reduce_lr = callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1,
    )

    history = model.fit(
        X_train_cnn,
        y_train,
        epochs=100,
        batch_size=16,
        validation_split=0.2,
        callbacks=[early_stop, reduce_lr],
        verbose=1,
    )

    # ----------------------------------------------------
    # 3-4. ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
    # ----------------------------------------------------
    if n_classes == 2:
        # ì´ì§„ ë¶„ë¥˜ â†’ sigmoid ì¶œë ¥
        y_pred_proba = model.predict(X_test_cnn).ravel()
        y_pred = (y_pred_proba >= 0.5).astype(int)
    else:
        # ë‹¤ì¤‘ ë¶„ë¥˜ â†’ softmax ì¶œë ¥
        y_pred_proba = model.predict(X_test_cnn)
        y_pred = np.argmax(y_pred_proba, axis=1)

    print("\n[ë¶„ë¥˜ ë¦¬í¬íŠ¸]")
    print(classification_report(y_test, y_pred))

    acc = accuracy_score(y_test, y_pred)
    print(f"[ì •í™•ë„] {acc * 100:.2f}%")

    # ----------------------------------------------------
    # 3-5. ì˜ˆì‹œ ì˜ˆì¸¡ 1ê±´ (í…ŒìŠ¤íŠ¸ì…‹ ê¸°ì¤€)
    # ----------------------------------------------------
    if len(X_test) > 0:
        sample_idx = 0
        true_label = int(y_test[sample_idx])
        pred_label = int(y_pred[sample_idx])

        sev_map = severity_text_by_psf.get(psf_id, default_severity_text)

        print("\n[ì˜ˆì‹œ ì˜ˆì¸¡ 1ê±´]")
        print(f"ì‹¤ì œ ë¼ë²¨: {true_label} ({sev_map.get(true_label, 'N/A')})")
        print(f"ì˜ˆì¸¡ ë¼ë²¨: {pred_label} ({sev_map.get(pred_label, 'N/A')})")
    else:
        print("\n[ì˜ˆì‹œ ì˜ˆì¸¡] í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì´ ì—†ì–´ ìƒëµ")

    # ğŸ”¹ ì°¸ê³ : LGBMì—ì„œ í•˜ë˜ Feature ImportanceëŠ”
    # CNNì—ëŠ” ê¸°ë³¸ ì œê³µë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” ìƒëµí–ˆìŠµë‹ˆë‹¤.
    # ë‚˜ì¤‘ì— permutation importance ê°™ì€ ê±¸ë¡œ ë”°ë¡œ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”.
